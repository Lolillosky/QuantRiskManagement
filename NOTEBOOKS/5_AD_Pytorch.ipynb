{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Risk Management\n",
    "\n",
    "Click <a href=\"https://colab.research.google.com/github/Lolillosky/QuantRiskManagement/blob/main/NOTEBOOKS/3_AD_Pytorch.ipynb\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d0/Google_Colaboratory_SVG_Logo.svg\" width=\"30\" alt=\"Google Colab\">\n",
    "</a> to open this notebook in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AD in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to install Pythorch in your machine follow the instructions from [Pythorch help](https://pytorch.org/). The library is already installed in Google Colab environment.\n",
    "\n",
    "In order to import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "In order to be able to compute derivatives, we have to work with Pytorch tensors. These can be initialized from hardcoded values, numpy variables or Pythorch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0)\n",
    "\n",
    "y_numpy = np.linspace(0,2*np.pi,10)\n",
    "\n",
    "y = torch.tensor(y_numpy)\n",
    "\n",
    "z = torch.linspace(0,2*np.pi,10)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The floating point precission can be set when a variable is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, dtype= torch.float64)\n",
    "\n",
    "y_numpy = np.linspace(0,2*np.pi,10)\n",
    "\n",
    "y = torch.tensor(y_numpy, dtype= torch.float64)\n",
    "\n",
    "z = torch.linspace(0,2*np.pi,10, dtype= torch.float64)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default floating point precision can alse be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to perform AD, we must specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "z = x**2 + x*y\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of basic operations (+,-*,/), we must use pytorch functions. Pytorch functions and usage resemble numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\n",
    "y = torch.sin(x)\n",
    "z = torch.sum(y)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "grad = x.grad\n",
    "\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), label = 'sin(x)')\n",
    "plt.plot(x.detach().numpy(), grad, label = r'$\\frac{d\\sin(x)}{dx}$')\n",
    "\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the numerical content of every tensor, but in AD has been enabled on the particular tensor, we must first detach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we compute gradients, the tape is deleted unless we tell Pytorch not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([1.5,3.25,3.47], requires_grad=True)\n",
    "\n",
    "z1 = torch.sum(x**2 - y**2)\n",
    "z2 = z1**2\n",
    "\n",
    "\n",
    "z1.backward()\n",
    "z2.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([1.5,3.25,3.47], requires_grad=True)\n",
    "\n",
    "z1 = torch.sum(x**2 - y**2)\n",
    "z2 = z1**2\n",
    "\n",
    "\n",
    "z1.backward(retain_graph=True)\n",
    "z2.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can disable tape recording without setting requires_grad to false:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\n",
    "\n",
    "y1 = torch.sin(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y2 = torch.sin(x)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian Matrix\n",
    "\n",
    "Let us first define a function that takes several inputs and outputs. For example, a formula that computes the Montecarlo price of both a call and a put option given a set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_payoffs(spot, strike, vol, r, div, ttm, num_sims):\n",
    "\n",
    "    brow = torch.tensor(np.random.normal(0,1,num_sims))*torch.sqrt(ttm)\n",
    "    \n",
    "    spot_mat = spot*torch.exp((r-div-0.5*vol*vol)*ttm + vol*brow)\n",
    "\n",
    "    call = torch.mean(torch.maximum(spot_mat - strike, torch.tensor(0.0)))*torch.exp(-r*ttm)\n",
    "    put = torch.mean(torch.maximum(-spot_mat + strike, torch.tensor(0.0)))*torch.exp(-r*ttm)\n",
    "\n",
    "    return (call, put)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a wrapper to this function, so that the wrapper only takes as inputs the parameters with respect to which we want to compute the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spot = torch.tensor(1.0, requires_grad=True)\n",
    "strike = torch.tensor(1.0, requires_grad=True)\n",
    "vol = torch.tensor(0.2, requires_grad=True)\n",
    "r = torch.tensor(0.01, requires_grad=True)\n",
    "div = torch.tensor(0.005, requires_grad=True)\n",
    "ttm = torch.tensor(1.0, requires_grad=True)\n",
    "num_sims = 50000\n",
    "\n",
    "MC_100000 = lambda spot, strike, vol, r, div, ttm: MC_payoffs(spot, strike, vol, r, div, ttm, 100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the Jacobian, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.5488),\n",
       "  tensor(-0.4667),\n",
       "  tensor(0.3971),\n",
       "  tensor(0.4667),\n",
       "  tensor(-0.5488),\n",
       "  tensor(0.0416)),\n",
       " (tensor(-0.4472),\n",
       "  tensor(0.5234),\n",
       "  tensor(0.3920),\n",
       "  tensor(-0.5234),\n",
       "  tensor(0.4472),\n",
       "  tensor(0.0362)))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "jacobian(MC_100000, (spot, strike, vol, r, div, ttm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\My_python\\Lib\\site-packages\\torch\\autograd\\__init__.py:266: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1182.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = 3*x**2-2.5*x+1\n",
    "\n",
    "first_der = y.backward(create_graph=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(0.3960),\n",
       "  tensor(1.1102e-16),\n",
       "  tensor(-0.5488),\n",
       "  tensor(0.0369)),\n",
       " (tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(0.),\n",
       "  tensor(0.4668),\n",
       "  tensor(0.),\n",
       "  tensor(0.0047)),\n",
       " (tensor(0.3960),\n",
       "  tensor(0.),\n",
       "  tensor(-0.0491),\n",
       "  tensor(5.5511e-17),\n",
       "  tensor(-0.3960),\n",
       "  tensor(0.1911)),\n",
       " (tensor(0.),\n",
       "  tensor(0.4668),\n",
       "  tensor(0.),\n",
       "  tensor(-0.4668),\n",
       "  tensor(-0.),\n",
       "  tensor(0.4622)),\n",
       " (tensor(-0.5488),\n",
       "  tensor(0.),\n",
       "  tensor(-0.3960),\n",
       "  tensor(-1.1102e-16),\n",
       "  tensor(0.5488),\n",
       "  tensor(-0.5857)),\n",
       " (tensor(0.0369),\n",
       "  tensor(0.0047),\n",
       "  tensor(0.1911),\n",
       "  tensor(0.4622),\n",
       "  tensor(-0.5857),\n",
       "  tensor(-0.0207)))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MC_100000_call = lambda spot, strike, vol, r, div, ttm: MC_payoffs(spot, strike, vol, r, div, ttm, 100000)[0]\n",
    "\n",
    "hessian(MC_100000_call, (spot, strike, vol, r, div, ttm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "definitive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

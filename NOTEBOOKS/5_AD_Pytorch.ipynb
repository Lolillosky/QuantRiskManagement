{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Risk Management\n",
    "\n",
    "Click <a href=\"https://colab.research.google.com/github/Lolillosky/QuantRiskManagement/blob/main/NOTEBOOKS/3_AD_Pytorch.ipynb\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/d/d0/Google_Colaboratory_SVG_Logo.svg\" width=\"30\" alt=\"Google Colab\">\n",
    "</a> to open this notebook in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to AD in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to install Pythorch in your machine follow the instructions from [Pythorch help](https://pytorch.org/). The library is already installed in Google Colab environment.\n",
    "\n",
    "In order to import the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "In order to be able to compute derivatives, we have to work with Pytorch tensors. These can be initialized from hardcoded values, numpy variables or Pythorch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0)\n",
    "\n",
    "y_numpy = np.linspace(0,2*np.pi,10)\n",
    "\n",
    "y = torch.tensor(y_numpy)\n",
    "\n",
    "z = torch.linspace(0,2*np.pi,10)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The floating point precission can be set when a variable is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, dtype= torch.float64)\n",
    "\n",
    "y_numpy = np.linspace(0,2*np.pi,10)\n",
    "\n",
    "y = torch.tensor(y_numpy, dtype= torch.float64)\n",
    "\n",
    "z = torch.linspace(0,2*np.pi,10, dtype= torch.float64)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default floating point precision can alse be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to perform AD, we must specify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "z = x**2 + x*y\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of basic operations (+,-*,/), we must use pytorch functions. Pytorch functions and usage resemble numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\n",
    "y = torch.sin(x)\n",
    "z = torch.sum(y)\n",
    "\n",
    "z.backward()\n",
    "\n",
    "grad = x.grad\n",
    "\n",
    "plt.plot(x.detach().numpy(), y.detach().numpy(), label = 'sin(x)')\n",
    "plt.plot(x.detach().numpy(), grad, label = r'$\\frac{d\\sin(x)}{dx}$')\n",
    "\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the numerical content of every tensor, but in AD has been enabled on the particular tensor, we must first detach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we compute gradients, the tape is deleted unless we tell Pytorch not to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([1.5,3.25,3.47], requires_grad=True)\n",
    "\n",
    "z1 = torch.sum(x**2 - y**2)\n",
    "z2 = z1**2\n",
    "\n",
    "\n",
    "z1.backward()\n",
    "z2.backward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0,2.0,3.0], requires_grad=True)\n",
    "\n",
    "y = torch.tensor([1.5,3.25,3.47], requires_grad=True)\n",
    "\n",
    "z1 = torch.sum(x**2 - y**2)\n",
    "z2 = z1**2\n",
    "\n",
    "\n",
    "z1.backward(retain_graph=True)\n",
    "z2.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can disable tape recording without setting requires_grad to false:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\n",
    "\n",
    "y1 = torch.sin(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y2 = torch.sin(x)\n",
    "\n",
    "print(y1)\n",
    "print(y2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Jacobian Matrix\n",
    "\n",
    "Let us first define a function that takes several inputs and outputs. For example, a formula that computes the Montecarlo price of both a call and a put option given a set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_payoffs(spot, strike, vol, r, div, ttm, num_sims):\n",
    "\n",
    "    brow = torch.tensor(np.random.normal(0,1,num_sims))*torch.sqrt(ttm)\n",
    "    \n",
    "    spot_mat = spot*torch.exp((r-div-0.5*vol*vol)*ttm + vol*brow)\n",
    "\n",
    "    call = torch.mean(torch.maximum(spot_mat - strike, torch.tensor(0.0)))*torch.exp(-r*ttm)\n",
    "    put = torch.mean(torch.maximum(-spot_mat + strike, torch.tensor(0.0)))*torch.exp(-r*ttm)\n",
    "\n",
    "    return (call, put)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a wrapper to this function, so that the wrapper only takes as inputs the parameters with respect to which we want to compute the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spot = torch.tensor(1.0, requires_grad=True)\n",
    "strike = torch.tensor(1.0, requires_grad=True)\n",
    "vol = torch.tensor(0.2, requires_grad=True)\n",
    "r = torch.tensor(0.01, requires_grad=True)\n",
    "div = torch.tensor(0.005, requires_grad=True)\n",
    "ttm = torch.tensor(1.0, requires_grad=True)\n",
    "num_sims = 50000\n",
    "\n",
    "MC_100000 = lambda spot, strike, vol, r, div, ttm: MC_payoffs(spot, strike, vol, r, div, ttm, 100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the Jacobian, do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "jacobian(MC_100000, (spot, strike, vol, r, div, ttm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High order differentials\n",
    "\n",
    "To compute high order differential, we must use torch.autograd.grad function iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = 3*x**3-2.5*x**2+x+1\n",
    "\n",
    "first_der = torch.autograd.grad(y,x,retain_graph= True, create_graph= True)\n",
    "\n",
    "print(first_der)\n",
    "\n",
    "second_der = torch.autograd.grad(first_der,x,retain_graph= True, create_graph= True)\n",
    "\n",
    "print(second_der)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "definitive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
